{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀人事異動情報を収集するためのツールです\n",
    "\n",
    "免責事項:\n",
    "当ツールによって生じた如何なるトラブル・損失・損害に対してもツール作成者は責任を負うものではないことを予めご了承ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ パラメータについて\n",
    "|#|パラメータ名|とりえる値|説明|\n",
    "|:--|:--:|:--:|:--|\n",
    "|1|TARGET_DATE|'1'~'31'|収集対象の人事情報の掲載日|\n",
    "|2|FI_WORDS|list[str]|企業が金融機関かどうかを判断するキーワード。この中のキーワードのいずれかが企業名に含まれていた場合に収集対象とする。|\n",
    "|3|GRCS_WORDS|list[str]|人事異動情報の詳細を表示するかどうか判断するキーワード。この中のキーワードのいずれかが人事異動情報に含まれていた場合に、結果に企業名だけでなく人事異動情報の詳細を出力する。|\n",
    "\n",
    "## 📖 使い方\n",
    "1. パラメータ3つを適切な値に設定する\n",
    "2. 実行\n",
    "3. 最下部に結果が表示される\n",
    "\n",
    "## 🔖 制約\n",
    "- 当日更新分は取得不可\n",
    "- 過去1−2週間程度のみ取得可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータ修正箇所↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DATE: str = '27'\n",
    "FI_WORDS = ['銀行', 'バンク', '金庫', '信金', '信用', '組合', 'ファイナンシャル', 'フィナンシャル', 'ファイナンス', '証券', '證券', '保険', '損保', '生命', '生保', '金融庁']\n",
    "GRCS_WORDS = ['IT', 'システム', 'リスク', '金融犯罪', 'AML', 'コンプライアンス', '犯罪', 'DX', 'デジタル']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml.html\n",
    "import time\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_target_date(target_date: str, content_date: str):\n",
    "    return '日' in content_date and target_date == content_date[0:len(target_date)]\n",
    "\n",
    "def is_financial_institution(title: str):\n",
    "    return any(fi_word in title for fi_word in FI_WORDS)\n",
    "\n",
    "def is_GRCS_related(article: str):\n",
    "    return any(grcs_word in article for grcs_word in GRCS_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL: str = 'https://www.nikkei.com'\n",
    "MAX_REQUESTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_SELECTOR = '#CONTENTS_MAIN > section > ul:nth-of-type({}) > li:nth-of-type({}) > div > div.col.headline > h3 > a'\n",
    "    ##CONTENTS_MAIN > section > ul:nth-child(2) > li:nth-child(1) > div > div.col.headline > h3 > a\n",
    "CONTENT_DATE_SELECTOR = '#CONTENTS_MAIN > section > ul:nth-of-type({}) > li:nth-of-type({}) > div > div.col.time > p'\n",
    "    ##CONTENTS_MAIN > section > ul:nth-child(2) > li:nth-child(1) > div > div.col.time > p\n",
    "HR_CONTENT_XPATH = '/html/body/div[5]/main/article/section[1]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "}\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> list:\n",
    "    stage = 1  # stage: 1-TARGET_DATEに達していない状態, 2-達した状態\n",
    "    result = []\n",
    "    cnt = 0  # cnt: ループの回数を記録する\n",
    "    for bn in range(1, 1982, 30):  # ページのURLが /?bn=31, /?bn=61のように30刻みとなっていることに対応\n",
    "        time.sleep(random.randint(1, 2))\n",
    "        soup = BeautifulSoup(session.get(BASE_URL+'/news/jinji/hatsurei/?bn='+str(bn), headers=HEADERS).text)\n",
    "        # print('[{}] Proceeding with Page-{}'.format(datetime.datetime.now(), bn // 30 + 1))\n",
    "        for ul_num in range(1, 4):\n",
    "            for li_num in range(1, 11):\n",
    "                cnt += 1\n",
    "                if cnt >= MAX_REQUESTS:  # 安全考慮\n",
    "                    result.append('Too many requests > {}'.format(MAX_REQUESTS))\n",
    "                    return result\n",
    "                header = soup.select(TITLE_SELECTOR.format(ul_num, li_num))\n",
    "                link, title = header[0].attrs['href'], header[0].contents[0].contents[0]\n",
    "\n",
    "                time_box = soup.select(CONTENT_DATE_SELECTOR.format(ul_num, li_num))\n",
    "                content_date = time_box[0].contents[0]\n",
    "                \n",
    "                print('\\r                                                                                 \\r', end='')\n",
    "                print('   [{}] Checking - ({}) {}'.format(cnt, content_date, title), end='', flush=True)\n",
    "\n",
    "                if is_target_date(TARGET_DATE, content_date):\n",
    "                    stage = 2\n",
    "                    if is_financial_institution(title):\n",
    "                        time.sleep(random.randint(1, 2))\n",
    "                        html = lxml.html.fromstring(session.get(BASE_URL+link, headers=HEADERS).text)\n",
    "                        hr_article = html.xpath(HR_CONTENT_XPATH)[0].text_content()\n",
    "                        if is_GRCS_related(hr_article):\n",
    "                            result.append('[{}]({}) ({}) > {}'.format(title, BASE_URL + link, content_date, hr_article))\n",
    "                        else:\n",
    "                            result.append('[{}]({}) ({}) '.format(title, BASE_URL + link, content_date))\n",
    "\n",
    "                else:\n",
    "                    if stage == 2:\n",
    "                        print('')\n",
    "                        return result\n",
    "    print('')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[{}] Starting crawl_hr_news_in_nikkei for TARGET_DATE = {}'.format(datetime.datetime.now(), TARGET_DATE))\n",
    "result = main()\n",
    "print('[{}] Completed crawl_hr_news_in_nikkei for TARGET_DATE = {}. Result follows...'.format(datetime.datetime.now(), TARGET_DATE))\n",
    "print('-----------------------------------------')\n",
    "if result:\n",
    "    for text in result:\n",
    "        print(text)\n",
    "else:\n",
    "    print('No result')\n",
    "print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4caafdc79b49e008fa20912fac537156b21852a028ad85ea4217a8a25d1d86b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
